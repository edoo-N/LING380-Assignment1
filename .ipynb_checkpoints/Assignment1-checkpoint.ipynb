{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af05205e-1198-4627-b854-2ba738299757",
   "metadata": {},
   "source": [
    "| Name (Last, First) | Student ID |           Section contributed           |   Section edited   |            Other contribution                |\r\n",
    "|:----------------:-  |-:------:- |:-------------------------------------:-|  :----------------:  |:------------------------------------------: \n",
    "| \r\n",
    "Natanael, Edward | d 30140286 | n GitHub setup/synci, function write up | code and write up a  Troubleshoot FreqDist, GitHub merge conflict   |\n",
    "| Supangat, Jonathan 301416826    Data Collection, function write up      | code and write up    Communication and task coordinationc        d| |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234914e-52ff-4d6f-a25c-bd733be178f5",
   "metadata": {},
   "source": [
    "External sources file URL:\n",
    "* Ghostbuster : https://www.scifiscripts.com/scripts/Ghostbusters.txt\n",
    "* Middlemarch : cleaned metadata from lab 4\n",
    "* Newspaper (EV Cars): https://www.cnn.com/interactive/2019/08/business/electric-cars-audi-volkswagen-tesla/, https://www.cnn.com/2019/08/01/cars/future-of-electric-car-charging/index.html, https://www.cnn.com/2019/08/07/business/ford-ceo-hackett-elon-musk-table-interview/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316d5c1-34bb-44af-813a-19072378b873",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4c8379c-dc3f-4b50-b1ce-3be1a64aac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import names\n",
    "# import the NLTK packages we know we need\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d2af5ec-f477-4ac0-8743-0a4b7b24258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spaCy and the small English language model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "# this does prettier displays on spaCy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "784394b2-9654-45c3-a3a8-02719939b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ff444b-fd41-4e97-9ddf-46af82247eb4",
   "metadata": {},
   "source": [
    "**Defining Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7945f317-30b3-4765-8a57-27b99a120004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_info(text):\n",
    "    \"\"\"\n",
    "    Uses NLTK to calculate: tokens, types, lexical diversity, Top 10 Frequent words\n",
    "    \n",
    "    Args:\n",
    "        text (str): a string containing the file or text\n",
    "        \n",
    "    Returns: \n",
    "        dict: a dictionary containing tokens, types, and lexical diversity, Top 10 Frequent words\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text.lower()) # Convert to lowercase for easy analysis\n",
    "    n_tokens = len(tokens)\n",
    "    n_types = len(set(tokens))\n",
    "    # counting lexical diversity\n",
    "    lex_div = n_types/len(tokens)\n",
    "    \n",
    "    # Create frequency distribution, without including punctuation\n",
    "    words = [word for word in tokens if word.isalnum()]\n",
    "    freqd = FreqDist(words) # punctuation removed\n",
    "    # Get the Top 10 used words \n",
    "    top10 = freqd.most_common(10)\n",
    "    return {\n",
    "            'number of tokens': n_tokens,\n",
    "            'types': n_types,\n",
    "            'lexical diversity': lex_div,\n",
    "            'Top 10 Freq Words': top10, \n",
    "        }\n",
    "\n",
    "\n",
    "def process_dir(path):\n",
    "    \"\"\"\n",
    "    Reads all the files in a directory. Processes them using the 'get_text_info' function\n",
    "    \n",
    "    Args: \n",
    "        path (str): path to the directory where the files are\n",
    "        \n",
    "    Returns:\n",
    "        dict: a dictionary with file names as keys and the tokens, types, lexical diversity, and top 10 frequent words as values\n",
    "    \n",
    "    \"\"\"\n",
    "    file_info = {}\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".txt\"):    \n",
    "            file_path = os.path.join(path, filename)      \n",
    "            with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                file_info[filename] = get_text_info(text)\n",
    "    return file_info\n",
    "\n",
    "def listentity(path):\n",
    "    '''\n",
    "    Reads all the files in a directory. Process them using SpaCy to extract list of named entities along with their labels, if present.\n",
    "    Args: \n",
    "        path (str) : path to the directory where the files are\n",
    "    \n",
    "    Retruns: data frame of named entities along with its label and filename for the three files in 'data' folder\n",
    "    \n",
    "    '''\n",
    "    # Creating a dictionary to store named entities for each of the files from path dir\n",
    "    named_entities = []\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".txt\"):    \n",
    "            file_path = os.path.join(path, filename)      \n",
    "            with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            # Defining NLP object   \n",
    "            doc = nlp(text) # Defining space to store spacy object for named entities\n",
    "\n",
    "            # go through the entities and append each to the list\n",
    "            for ent in doc.ents:\n",
    "                named_entities.append((filename,ent.text, ent.label_))\n",
    "\n",
    "            # create a df for the entities, from the list above \n",
    "            df_ents = pd.DataFrame(named_entities)\n",
    "            # name the columns\n",
    "            df_ents.columns = ['Filename','Entity', 'Label']\n",
    "    return df_ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f48478-e3e5-4632-8888-b3fe61e5b592",
   "metadata": {},
   "source": [
    "**Reading Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d63f3da1-04c6-47e6-b6bc-e61768559c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path. This directory should have more than 1 file\n",
    "path = './rawdata'\n",
    "\n",
    "files_in_dir_info = process_dir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d6ac49-12c8-49ad-a8b3-e58c7129b061",
   "metadata": {},
   "source": [
    "**Analyzing Contents I**\n",
    "\n",
    "The analysis below containing each of the three '.txt' file in the 'data' directory:\n",
    "* Number of tokens\n",
    "* Number of types\n",
    "* Lexical Diversity\n",
    "* Top 10 Frequent used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5b57a2e-a209-48d8-b225-d1fd683fe0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'EVCars.txt': {'number of tokens': 6038,\n",
       "  'types': 1483,\n",
       "  'lexical diversity': 0.2456111295130838,\n",
       "  'Top 10 Freq Words': [('the', 278),\n",
       "   ('to', 149),\n",
       "   ('and', 123),\n",
       "   ('of', 122),\n",
       "   ('a', 105),\n",
       "   ('in', 84),\n",
       "   ('that', 73),\n",
       "   ('is', 68),\n",
       "   ('s', 67),\n",
       "   ('electric', 62)]},\n",
       " 'Ghostbusters.txt': {'number of tokens': 29380,\n",
       "  'types': 4223,\n",
       "  'lexical diversity': 0.14373723621511234,\n",
       "  'Top 10 Freq Words': [('the', 1692),\n",
       "   ('and', 675),\n",
       "   ('a', 615),\n",
       "   ('to', 532),\n",
       "   ('of', 440),\n",
       "   ('venkman', 417),\n",
       "   ('i', 372),\n",
       "   ('you', 367),\n",
       "   ('it', 323),\n",
       "   ('in', 278)]},\n",
       " 'Middlemarch.txt': {'number of tokens': 373877,\n",
       "  'types': 19342,\n",
       "  'lexical diversity': 0.051733591528764805,\n",
       "  'Top 10 Freq Words': [('the', 12679),\n",
       "   ('to', 10158),\n",
       "   ('of', 8980),\n",
       "   ('and', 8497),\n",
       "   ('a', 7257),\n",
       "   ('in', 5287),\n",
       "   ('that', 5056),\n",
       "   ('he', 4943),\n",
       "   ('was', 4609),\n",
       "   ('i', 4589)]}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_in_dir_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9c8105a-6eec-4cc3-bd72-f38ab10a2e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number of tokens</th>\n",
       "      <th>types</th>\n",
       "      <th>lexical diversity</th>\n",
       "      <th>Top 10 Freq Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EVCars.txt</th>\n",
       "      <td>6038</td>\n",
       "      <td>1483</td>\n",
       "      <td>0.245611</td>\n",
       "      <td>[(the, 278), (to, 149), (and, 123), (of, 122),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ghostbusters.txt</th>\n",
       "      <td>29380</td>\n",
       "      <td>4223</td>\n",
       "      <td>0.143737</td>\n",
       "      <td>[(the, 1692), (and, 675), (a, 615), (to, 532),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Middlemarch.txt</th>\n",
       "      <td>373877</td>\n",
       "      <td>19342</td>\n",
       "      <td>0.051734</td>\n",
       "      <td>[(the, 12679), (to, 10158), (of, 8980), (and, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  number of tokens  types  lexical diversity  \\\n",
       "EVCars.txt                    6038   1483           0.245611   \n",
       "Ghostbusters.txt             29380   4223           0.143737   \n",
       "Middlemarch.txt             373877  19342           0.051734   \n",
       "\n",
       "                                                  Top 10 Freq Words  \n",
       "EVCars.txt        [(the, 278), (to, 149), (and, 123), (of, 122),...  \n",
       "Ghostbusters.txt  [(the, 1692), (and, 675), (a, 615), (to, 532),...  \n",
       "Middlemarch.txt   [(the, 12679), (to, 10158), (of, 8980), (and, ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(files_in_dir_info, orient='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37483d5-61bb-437d-a4a3-f12aacb0b22f",
   "metadata": {},
   "source": [
    "**Analyzing Content II**\n",
    "\n",
    "Below is the function call to display the 3 files named entities and their labels, if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "319e2846-726d-4af1-9af9-73b2c3b4db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling function to display the dataframe containing list of named entities and their labels\n",
    "named_entity_df = listentity(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0a54dbf-2064-407f-8d4c-1b32e9a95e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['EVCars.txt', 'Ghostbusters.txt', 'Middlemarch.txt'], dtype=object),\n",
       " array([  463,  1358, 12506], dtype=int64))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether all three files present in the data frame \n",
    "np.unique(named_entity_df.Filename, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bc6a863-3c52-4a80-9f68-db4c4b3184cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Filename              Entity     Label\n",
      "0           EVCars.txt            Brussels       GPE\n",
      "1           EVCars.txt             Germany       GPE\n",
      "2           EVCars.txt    Volkswagen Group       ORG\n",
      "3           EVCars.txt          Thirty-six  CARDINAL\n",
      "4           EVCars.txt               dozen  CARDINAL\n",
      "...                ...                 ...       ...\n",
      "14322  Middlemarch.txt              eBooks       ORG\n",
      "14323  Middlemarch.txt   Project Gutenberg       ORG\n",
      "14324  Middlemarch.txt  Archive Foundation       ORG\n",
      "14325  Middlemarch.txt              eBooks       ORG\n",
      "14326  Middlemarch.txt              eBooks       ORG\n",
      "\n",
      "[14327 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(named_entity_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f670c8f-4700-44b5-838d-19d41c11a67a",
   "metadata": {},
   "source": [
    "**Short Reflection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa0eea3-4400-4bb9-9931-4437a9d4d724",
   "metadata": {},
   "source": [
    "a) What do the most frequent words tell you about each of the 3 genres? \n",
    "-> Based on the most frequent words results, we could see that each of the 3 genres have similar frequent usage of words between them. The most notable difference would be 'Venkman' for Ghostbusters script and 'Electric' for EV Cars Newspaper article. Based on this, we feel that this provide a differentiation between the content, audience, and mode of delivery that each of the 3 genres have.\n",
    "\n",
    "b) What do the named entities tell you about each of the 3 genres?\n",
    "-> The named entities output that we got for each of the 3 genres really showed the differentiation of the 3 genres more than the most frequent words. Because of the correct named entities and their correct labeling, we could see that EVCars.txt for example to be a newspaper article as it has unique named entity like Volkswagen Group (ORG labeled) for example showing that it is a vehicle organization group. And as for Middlemarch.txt we could also know that it is a book as provided details from the named entity of eBooks (ORG Labeled) showing that it is a electronic books organization.  \n",
    "\n",
    "c) Is the named entity output correct, in your opinion? If not, why not?\n",
    "-> Yes, we would say that the named entity output of our assignment is correct. Because it shows the different entities that is available from each of the 3 genres and provided correct labeling. Where for example, in EV Cars Newspaper article it has Brussels, Volkswagen Group, and Dozen entities and they are labeled correctly. This is important because from having the correct named entity output and labeling we could see the differentiation between the 3 genres. \n",
    "\n",
    "d) How do you interpret the lexical diversity?\n",
    "-> EV Cars Newspaper article has a lexical diversity of 0.245611, which indicates that they have a higher lexical diversity than the other 2 genres. Therefore, it suggests that this genre has more unique words which is as expected because this genre use more technical terms and a richer vocabulary as it is a newspaper article about electric vehicles. \n",
    "-> Ghostbusters movie script has a lexical diversity of 0.143737, which indicates that they have a lower lexical diversity than the EV Cars Newspaper article but still higher than the Middlemarch book. Therefore, the lexical diversity is quite moderate and it suggests that this genre is dialogue heavy and have repetition of function words as it is expected in movie scripts in general. \n",
    "-> Middlemarch book has a lexical diversity of 0.051734, which indicates that this genre have the lowest lexical diversity than the other 2 genres. Therefore, it suggests that this genre has a very long story with a high repetition use of common words and phrases. Which is also as expected in the books genre in general. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ae4f9f-b86a-40e6-83a8-4a90c0b98175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
