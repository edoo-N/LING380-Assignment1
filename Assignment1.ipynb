{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af05205e-1198-4627-b854-2ba738299757",
   "metadata": {},
   "source": [
    "| Name (Last, First) | Student ID |           Section contributed           |   Section edited   |            Other contribution                |\r\n",
    "|:----------------:-  |-:------:- |:-------------------------------------:-|  :----------------:  |:------------------------------------------: \n",
    "| \r\n",
    "Natanael, Edward | d 30140286 | n GitHub setup/synci, function write up | code and write up a  Troubleshoot FreqDist, GitHub merge conflict   |\n",
    "| Supangat, Jonathan 301416826    Data Collection, function write up      | code and write up    Communication and task coordinationc        d| |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a234914e-52ff-4d6f-a25c-bd733be178f5",
   "metadata": {},
   "source": [
    "External sources file URL:\n",
    "* Ghostbuster : https://www.scifiscripts.com/scripts/Ghostbusters.txt\n",
    "* Middlemarch : cleaned metadata from lab 4\n",
    "* Newspaper (EV Cars): https://www.cnn.com/interactive/2019/08/business/electric-cars-audi-volkswagen-tesla/, https://www.cnn.com/2019/08/01/cars/future-of-electric-car-charging/index.html, https://www.cnn.com/2019/08/07/business/ford-ceo-hackett-elon-musk-table-interview/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316d5c1-34bb-44af-813a-19072378b873",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c8379c-dc3f-4b50-b1ce-3be1a64aac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import names\n",
    "# import the NLTK packages we know we need\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d2af5ec-f477-4ac0-8743-0a4b7b24258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spaCy and the small English language model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 2000000\n",
    "\n",
    "# this does prettier displays on spaCy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "784394b2-9654-45c3-a3a8-02719939b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ff444b-fd41-4e97-9ddf-46af82247eb4",
   "metadata": {},
   "source": [
    "**Defining Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7945f317-30b3-4765-8a57-27b99a120004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_info(text):\n",
    "    \"\"\"\n",
    "    Uses NLTK to calculate: tokens, types, lexical diversity, Top 10 Frequent words\n",
    "    \n",
    "    Args:\n",
    "        text (str): a string containing the file or text\n",
    "        \n",
    "    Returns: \n",
    "        dict: a dictionary containing tokens, types, and lexical diversity\n",
    "    \"\"\"\n",
    "    doc = nlp(text) # Defining space to store spacy object for named entities\n",
    "    tokens = nltk.word_tokenize(text.lower()) # Convert to lowercase for easy analysis\n",
    "    n_tokens = len(tokens)\n",
    "    n_types = len(set(tokens))\n",
    "    # counting lexical diversity\n",
    "    lex_div = n_types/len(tokens)\n",
    "    \n",
    "    # Create frequency distribution, without including punctuation\n",
    "    words = [word for word in tokens if word.isalnum()]\n",
    "    freqd = FreqDist(words) # punctuation removed\n",
    "    # Get the Top 10 used words \n",
    "    top10 = freqd.most_common(10)\n",
    "    return {\n",
    "            'number of tokens': n_tokens,\n",
    "            'types': n_types,\n",
    "            'lexical diversity': lex_div,\n",
    "            'Top 10 Freq Words': top10, \n",
    "            'Named entities': doc \n",
    "        }\n",
    "\n",
    "\n",
    "def process_dir(path):\n",
    "    \"\"\"\n",
    "    Reads all the files in a directory. Processes them using the 'get_text_info' function\n",
    "    \n",
    "    Args: \n",
    "        path (str): path to the directory where the files are\n",
    "        \n",
    "    Returns:\n",
    "        dict: a dictionary with file names as keys and the tokens, types, lexical diversity, as values\n",
    "    \n",
    "    \"\"\"\n",
    "    file_info = {}\n",
    "\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".txt\"):    \n",
    "            file_path = os.path.join(path, filename)      \n",
    "            with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "                file_info[filename] = get_text_info(text)\n",
    "    return file_info\n",
    "\n",
    "def listentity(files):\n",
    "    '''\n",
    "    '''\n",
    "    # I am creating a dictionary to store named entities for each of the files from path dir\n",
    "    named_entities = {}\n",
    "    \n",
    "    for filename, info in files.items():\n",
    "        doc = info[\"doc\"]\n",
    "        entities = [ent.text for ent in doc.ents]  # This is to extract the entity texts\n",
    "        named_entities[filename] = entities\n",
    "    \n",
    "        # Print the named entities for each file\n",
    "        print(f\"File: {filename}\")\n",
    "        print(f\"Named Entities: {entities}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f48478-e3e5-4632-8888-b3fe61e5b592",
   "metadata": {},
   "source": [
    "**Reading Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63f3da1-04c6-47e6-b6bc-e61768559c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path. This directory should have more than 1 file\n",
    "path = './rawdata'\n",
    "\n",
    "files_in_dir_info = process_dir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d6ac49-12c8-49ad-a8b3-e58c7129b061",
   "metadata": {},
   "source": [
    "**Analyzing Contents**\n",
    "\n",
    "The analysis below containing each of the three '.txt' file in the 'data' directory:\n",
    "* Number of tokens\n",
    "* Number of types\n",
    "* Lexical Diversity\n",
    "* Top 10 Frequent used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b57a2e-a209-48d8-b225-d1fd683fe0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in_dir_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c8105a-6eec-4cc3-bd72-f38ab10a2e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(files_in_dir_info, orient='index')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319e2846-726d-4af1-9af9-73b2c3b4db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "listentity(files_in_dir_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f670c8f-4700-44b5-838d-19d41c11a67a",
   "metadata": {},
   "source": [
    "**Short Reflection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d54a46-bc2a-476d-b075-98e61c4ef804",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
